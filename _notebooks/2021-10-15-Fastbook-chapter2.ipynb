{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mldurga/easydl/blob/master/_notebooks/2021-10-15-Fastbook-chapter2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzlZDn2MlEFJ"
      },
      "source": [
        "# Fastai ```fit_one_cycle``` & ```fine_tune``` : Super-Convergence (Leslie Smith)  \n",
        "> Exploring Source code of fastai\n",
        "\n",
        "- toc: true\n",
        "- branch: master\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [paper_reading]\n",
        "- image: images/some_folder/your_image.png\n",
        "- hide: true\n",
        "- search_exclude: true\n",
        "- metadata_key1: metadata_value1\n",
        "- metadata_key2: metadata_value2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtNN1VF6pWRc"
      },
      "source": [
        "##Topology of Loss function\n",
        "Nothing is more explainable than this \n",
        "\n",
        "> twitter: https://twitter.com/zozuar/status/1443012484189888515?s=20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grw3fac2lJP_"
      },
      "source": [
        "## Introduction\n",
        "Until publications of the papers [1](https://arxiv.org/pdf/1506.01186.pdf),[2](https://arxiv.org/pdf/1803.09820.pdf),[3](https://arxiv.org/pdf/1708.07120.pdf) by Mr Leslie Smith, finding learning rate of neural networks training is largely a black art and often requires extensive practice and expertise to set good hyperparameters. Mr Smith's contribution greatly helped reserach community on overall understanding of the topology of loss function. Unlike typical Machine Learning papers, Smith's Papers are very much approachable and invaluable. I have tried to reproduce fastai source code of implementation of 1cycle policy inspired from Smith's papers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdm7uz-4p7uB"
      },
      "source": [
        "##Learning rate and Weight updataion\n",
        "In neural networks training, loss function (simply distance between target value and predicted value) must be decreased with each iteration. SGD - stochastic gradient descent is the method to achieve the above stated goal. If you can observe the above twitter card, loss function topology will be more or less have same features. Identifying global minimum of that topology is the sole aim of any Machine learning practitioner. However, training gets trapped over local minima and model underperforms. This is where Smiths ideas catapulted training neural networks to the next level. "
      ]
    }
  ]
}