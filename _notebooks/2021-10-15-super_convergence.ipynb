{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mldurga/easydl/blob/master/_notebooks/2021-10-15-super_convergence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QzD-faO9zPP"
      },
      "source": [
        "# \"Fastai ```fit_one_cycle``` & ```fine_tune``` and Super-Convergence (Leslie Smith)\"  \n",
        "> Exploring Source code of fastai\n",
        "\n",
        "- toc: true\n",
        "- branch: master\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [paper_reading]\n",
        "- image: images/some_folder/your_image.png\n",
        "- hide: true\n",
        "- search_exclude: true\n",
        "- metadata_key1: metadata_value1\n",
        "- metadata_key2: metadata_value2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNWfTNCyBIUN"
      },
      "source": [
        "##Topology of Loss function\n",
        "Nothing is more explainable than this \n",
        "\n",
        "> twitter: https://twitter.com/zozuar/status/1443012484189888515?s=20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No6xb0XwBLwt"
      },
      "source": [
        "## Introduction\n",
        "Until publications of the papers [1](https://arxiv.org/pdf/1506.01186.pdf),[2](https://arxiv.org/pdf/1803.09820.pdf),[3](https://arxiv.org/pdf/1708.07120.pdf) by Mr Leslie Smith, finding learning rate of neural networks training is largely a black art and often requires extensive practice and expertise to set good hyperparameters. Mr Smith's contribution greatly helped reserach community on overall understanding of the topology of loss function. Unlike typical Machine Learning papers, Smith's Papers are very much approachable and invaluable. I have tried to reproduce fastai source code of implementation of 1cycle policy inspired from Smith's papers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVEmMgqSBO18"
      },
      "source": [
        "##Learning rate and Weight updataion\n",
        "In neural networks training, loss function (simply distance between target value and predicted value) must be decreased with each iteration. SGD - stochastic gradient descent is the method to achieve the above stated goal. If you can observe the above twitter card, loss function topology will be more or less have same features. Identifying global minimum of that topology is the sole aim of any Machine learning practitioner. However, training gets trapped over local minima and model underperforms. This is where Smiths ideas catapulted training neural networks to the next level. \n",
        "\n",
        "![image](https://user-images.githubusercontent.com/19243618/137064877-7a3dae9f-496a-4847-b11c-56d0aadb6283.png)\n",
        "\n",
        "```w.new=w.old - gradient(w.old) * lr```\n",
        "\n",
        "above image gives general idea of Lerning Rate value impact on convergence of loss function. weights will get updated for each iteration at the rate of choosen LR. Low LR can have effect of trapping at [saddle](https://en.wikipedia.org/wiki/Saddle_point) points and local minima, High LR can immediately be diverged and loss value increases greatly. Until Smith's papers, setting right LR has been a great challenge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88UgFXMzBnuo"
      },
      "source": [
        "## Cyclical Learning Rate (CLR)\n",
        "Conventionally LR is a fixed value and tends to be decreased over a period of training until a good accuracy is obtained. Smith introduced novel technique of varying learning rate in cyclical fashion resulted in huge gains in accuracy in few iterations compared to the conventional practices.\n",
        "\n",
        "![cifar_LR](https://user-images.githubusercontent.com/19243618/137067445-bef503ca-dc77-447b-88e7-ba6126908b2d.jpg)\n",
        "\n",
        "the CLR approach has clearly outperformed other techniques of setting LR as hown in figure. CLR achieved same accuracy within 20k iterations compared to 80k iterations of other techniques.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9KyxsMzBray"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/19243618/137078413-04295f10-8a72-4348-895d-2c4786bb847a.png)\n",
        "\n",
        "LR has to be varied between two bounds called maximum bound and minimum bound. These can be found from LR range test, which is identifying minimum and maximum learning rate values where accuracy is actually increasing, by doing few iteratrions of training. Due to simplicity, triangular learning rate policy was adopted for most of the cases. following terminology helps in understanding setting of learning rate.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMLdsGivSLe9"
      },
      "source": [
        "\n",
        "*  epoch: one pass through of the entire dataset\n",
        "*  batch size: No of images in a mini batch\n",
        "*  iteration: training over a mini batch\n",
        "\n",
        "Lets take 50,000 images to be trained and with a 100 images per minibatch \n",
        "\n",
        "\n",
        "*   No of iterations per epoch => (50000/100) =500\n",
        "*   stepsize : Generally 2 times epoch, so every 1000 iterations we have one increaing learning rate and next 1000 iterations we have one decreasing learning rate, this becomes one cycle\n",
        "\n",
        "Smiths experiments shows that generally 3 cycles have trained the network within short iterations compared to conventional LR methods. However 1cycle [paper](https://arxiv.org/pdf/1708.07120.pdf) in 2018 stated that one cycle is sufficient to train the network when very large learning rates are used.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxxuioCZmeIQ"
      },
      "source": [
        "## Super-Convergence\n",
        "Cyclic learning rates coupled with large values of learning rates leads to convergence much faster than standard training methods. The large learning rates allows training to jump over the local minimas and will find a way to global minima in much less time. Cyclical nature of the learning rates enables training to first slowly converge then moves at faster speeds to traverse the valley and again slows the pace to finally converge to final accuracy level. \n",
        "\n",
        "![image](https://user-images.githubusercontent.com/19243618/137189466-0b3eca30-9ad9-4e5b-9cf6-6b1c4deb5be9.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oZ4p-Gw2yBg"
      },
      "source": [
        "Above images are from super-convergence paper. Figure (a) clearly shows the dominance of Super-Convergence approach denoted by CLR. With CLR range 0.1-3 (Learning rate bounds) accuracy obtained 92.4% within 10k iterations compared to 80k iterations of normal training methods on CIFAR-10 Dataset, ResNet-56 architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cn9xt3N4wmK"
      },
      "source": [
        "To reiterate author words \"Always use one cycle that is smaller than the total number of iterations/epochs and allow the learning rate to decrease several orders of magnitude less than the initial learning rate for the remaining iterations\". This is called 1cycle policy or Super-Convergence. Since 1cycle policy have regularisation effect on overall training process, care must be taken to balance the remianing regularisation techniques like weight-decay, batch size, drop out etc.. depending on the dataset.\n",
        "\n",
        "\n"
      ]
    }
  ]
}