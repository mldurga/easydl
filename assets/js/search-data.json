{
  
    
        "post0": {
            "title": "Missing semester MIT Exercise 2 Solutions",
            "content": ". MIT Missing semester Lectures are greatly helps one to work with terminal, git -version control systems, data wrangling etc... Here I have listed out solutions for the exercises of each lecture. . Read man ls and write an ls command that lists files in the following manner Includes all files, including hidden files Sizes are listed in human readable format (e.g. 454M instead of 454279954) Files are ordered by recency Output is colorized | !ls -laht --color=always . total 28K drwxr-xr-x 1 root root 4.0K Aug 26 06:56 . drwxr-xr-x 2 root root 4.0K Aug 26 06:56 bar drwxr-xr-x 2 root root 4.0K Aug 26 06:56 foo drwxr-xr-x 2 root root 4.0K Aug 26 06:56 fooob drwxr-xr-x 1 root root 4.0K Aug 26 06:41 .. drwxr-xr-x 1 root root 4.0K Aug 13 13:35 sample_data drwxr-xr-x 4 root root 4.0K Aug 13 13:34 .config . Write bash functions marco and polo that do the following. Whenever you execute marco the current working directory should be saved in some manner, then when you execute polo, no matter what directory you are in, polo should cd you back to the directory where you executed marco. For ease of debugging you can write the code in a file marco.sh and (re)load the definitions to your shell by executing source marco.sh. | touch marco.sh polo.sh chmod +x marco.sh polo.sh marco.sh marco () { pwd &gt; marco.txt } polo.sh polo () { cd $(cat &lt; marco.txt) } . after creating marco.sh and polo.sh files in vim call them from anywhere using source command . source path/to/workingdir/marco.sh source path/to/workingdir/polo.sh . Say you have a command that fails rarely. In order to debug it you need to capture its output but it can be time consuming to get a failure run. Write a bash script that runs the following script until it fails and captures its standard output and error streams to files and prints everything at the end. Bonus points if you can also report how many runs it took for the script to fail. |",
            "url": "https://mldurga.github.io/easydl/2021/08/26/MIT_exercise_2.html",
            "relUrl": "/2021/08/26/MIT_exercise_2.html",
            "date": " • Aug 26, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Images downloading methods",
            "content": "Following are the ways I used to download images from internet for data analysis purpose . Using search_image_ddg | Using Pyimage search method | . search_image_ddg method . From fastbook webiste you can see the documentaion, however here I repordice code snippet for easy usage. . !pip install fastbook -Uqq import fastbook fastbook.setup_book() . Mounted at /content/gdrive . from fastbook import * . def search_images_ddg(term, max_images=200): # search_image_ddg in fastbook version has some issues so defining seperately &quot;Search for `term` with DuckDuckGo and return a unique urls of about `max_images` images&quot; assert max_images&lt;1000 url = &#39;https://duckduckgo.com/&#39; res = urlread(url,data={&#39;q&#39;:term}) searchObj = re.search(r&#39;vqd=([ d-]+) &amp;&#39;, res) assert searchObj requestUrl = url + &#39;i.js&#39; params = dict(l=&#39;us-en&#39;, o=&#39;json&#39;, q=term, vqd=searchObj.group(1), f=&#39;,,,&#39;, p=&#39;1&#39;, v7exp=&#39;a&#39;) urls,data = set(),{&#39;next&#39;:1} while len(urls)&lt;max_images and &#39;next&#39; in data: try: data = urljson(requestUrl,data=params) urls.update(L(data[&#39;results&#39;]).itemgot(&#39;image&#39;)) requestUrl = url + data[&#39;next&#39;] except (URLError,HTTPError): pass time.sleep(0.2) return L(urls) . results = search_images_ddg(&#39;search word&#39;, max_images=300) . download_images(dest, urls=results) #destination folder path . Using pyimage search method . Pyimage search is well known for computer vision tutorials and Mr Adrian Rosebrock has given code snippet in his blog here . You only need to copy the following code snippet into the javascript console and type enter. Text file will be downloaded comprising all urls related to the searched images. javascript console will be opened in browser by pressing F12 key in either mozilla firefox or chrome. . function simulateRightClick( element ) { var event1 = new MouseEvent( &#39;mousedown&#39;, { bubbles: true, cancelable: false, view: window, button: 2, buttons: 2, clientX: element.getBoundingClientRect().x, clientY: element.getBoundingClientRect().y } ); element.dispatchEvent( event1 ); var event2 = new MouseEvent( &#39;mouseup&#39;, { bubbles: true, cancelable: false, view: window, button: 2, buttons: 0, clientX: element.getBoundingClientRect().x, clientY: element.getBoundingClientRect().y } ); element.dispatchEvent( event2 ); var event3 = new MouseEvent( &#39;contextmenu&#39;, { bubbles: true, cancelable: false, view: window, button: 2, buttons: 0, clientX: element.getBoundingClientRect().x, clientY: element.getBoundingClientRect().y } ); element.dispatchEvent( event3 ); } function getURLParam( queryString, key ) { var vars = queryString.replace( /^ ?/, &#39;&#39; ).split( &#39;&amp;&#39; ); for ( let i = 0; i &lt; vars.length; i++ ) { let pair = vars[ i ].split( &#39;=&#39; ); if ( pair[0] == key ) { return pair[1]; } } return false; } function createDownload( contents ) { var hiddenElement = document.createElement( &#39;a&#39; ); hiddenElement.href = &#39;data:attachment/text,&#39; + encodeURI( contents ); hiddenElement.target = &#39;_blank&#39;; hiddenElement.download = &#39;urls.txt&#39;; hiddenElement.click(); } function grabUrls() { var urls = []; return new Promise( function( resolve, reject ) { var count = document.querySelectorAll( &#39;.isv-r a:first-of-type&#39; ).length, index = 0; Array.prototype.forEach.call( document.querySelectorAll( &#39;.isv-r a:first-of-type&#39; ), function( element ) { // using the right click menu Google will generate the // full-size URL; won&#39;t work in Internet Explorer // (http://pyimg.co/byukr) simulateRightClick( element.querySelector( &#39;:scope img&#39; ) ); // Wait for it to appear on the &lt;a&gt; element var interval = setInterval( function() { if ( element.href.trim() !== &#39;&#39; ) { clearInterval( interval ); // extract the full-size version of the image let googleUrl = element.href.replace( /.*( ?)/, &#39;$1&#39; ), fullImageUrl = decodeURIComponent( getURLParam( googleUrl, &#39;imgurl&#39; ) ); if ( fullImageUrl !== &#39;false&#39; ) { urls.push( fullImageUrl ); } // sometimes the URL returns a &quot;false&quot; string and // we still want to count those so our Promise // resolves index++; if ( index == ( count - 1 ) ) { resolve( urls ); } } }, 10 ); } ); } ); } grabUrls().then( function( urls ) { urls = urls.join( &#39; n&#39; ); createDownload( urls ); } ); . After downloading the urls text file, upload to the jupyter notebook or download the images just as before . download_images(dest=&#39;./file&#39;, urls=Path(&#39;/content/urls.txt&#39;)) .",
            "url": "https://mldurga.github.io/easydl/2021/08/25/ImagesDownload.html",
            "relUrl": "/2021/08/25/ImagesDownload.html",
            "date": " • Aug 25, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post test check check",
            "content": "Play with Beard First ! . It is very exciting for me to share with you the recent experiments with deep learning after introduced to fast.ai. Just open this link and upload any modi (Indian PM) photo from 2014 to 2021 (you know difference is beard :smiley:), my model will predict whether its from 2014 or 2021 with nearly 93% accuracy. Journey into deep learning is fun and exhilarating. I seriously urge you to look at fast.ai if you are interested in deep learning. Now, lets dive into my blog. . What is Fastai . Fastai is front runner and research organisation working on making Deep Learning as democratic as possible with beautiful community support. Fastai in a way cutshorts the path needed for one to become a great ML practitioner and honestly stands to the title of its popular book: Deep Learning for coders with fastai &amp; PyTorch (AI Applicaitons without a PhD). I sincearly want to thank Mr Jeremy Howard and Mr Sylvain Gugger for presenting such an awesome gem in just 600 pages. The series of posts that I am going to write on this book is in a way to solidfy my understanding of the concepts taught in the book and practice writing as encouraged greatly by authors and community. I hope this turns out to be good source to refer for me and beginners who are just behind me. . What tools do you need . Mind you we are just beginners and we dont need anything more than Google colab scratchpad. This will spinup notebook with free GPU (Graphical Processing Unit) for you with just click of a mouse. Great colab tips can be found here. Full server notebooks like GCP and AWS instances are still difficult to setup for complete beginners like me. Believe me, I have spent one whole month in understanding how to setup GCP instance. Now almost all the time I spinup colab for any course work for its ease and simplicity. People like Zachery Mueller, who is like poster child for fastai community, prefers to practice with colab. . Domains of Deep Learning . Deep learning can be applied to wide variety of tasks and tinkering mindset can force one to enable applying it in creative ways. The increasing availabilty of data in any field will provide possible use case and can leverage new insights. However, the current research and practice broadly falls under following categories. . Computer Vision: . What do you think Tesla FSD (Full Self Driving) cars equipped with for its Autopilot system? Computer vision plays central bedrock role for autonomous driving. Today computers are as good as people in identifying objects in an image using their neural networks. We take up computer vision in this chapter and slowly expand to other domains as course progress. Any deep learning system predictions depend on the quality of the training data that we provide. Certainly this book provides tools and strategies to get state of the art accuracy with innovative methods even with limited data. . Natural Language Processing NLP: . Did you ever hear GPT-3, GPT-2, Transformers, Huggingface, twitter bots, google translate etc. in deep learning space, then all these terms belong to text domain of deep learning. This domain deals with how computers handles text data and its response for a specified task. Today NLP tasks ranges from classifying documents, sentiment analysis (positive, negetive), summarising documents, context appropriate text generation etc. Downside of technology ranges from &#39;throwing abuses by bot&#39; to &#39;bot generated twitter trolls at massive scale&#39;. However, Google translate massively used by people in ever increasing multicultural society of today. . Tabular Data &amp; RecSys: . Most common ML applications we find at industry is tabular data based. Though traditional ML techniques like random forests and gradient boost techniques still hold good, deep learning offlate making strides into tabular data domain. RecSys - Recommendation System is also type of tabular data but the data is highly cardinal and deep learning can be applied to such data with good results. E-commerce websites like amazon applies deep learning techniques for its user-table to recommend products. . Thinking Approach . When solving any problem at hand with ML and deep learning techniques, the fundamental thinking approach should be the predictions of the model should be helpful to the real world. Jeremy Howard named this approach as Drivetrain Approach and his elegant presentation can be seen here. His key ideas are as follows: . Objective: What outcome am I trying to achieve | Levers: What inputs can we control | Data: What data can we collect | Models: How the levers influence the objective | Thouogh seemingly simple concept, its application in real world has great impact on the outcome of the modeling. Ingeneral the data we have on hand may not be causal and the outcomes may not represent the actual conditions. If we can control certain input conditions by injecting randomness the causal relationships can be collected and new data will become food for the model and thus accuracy of the predictions can be improved. . Lets Dive In . To make your first dive in experience into deep learning little more interesting, I have choosen a &quot;Modi&#39;s Beard&quot; project. Interesting..! yes. You upload any of the present Indian PM &quot;Narendra modi&quot; image, my app will tell whether the image is &quot;2014 Modi&quot; or &quot;2021 Modi&quot;. This we are doing with computer vision deep learning techniques. This notebook will detail each and every step of the process. . Gathering Data . Though &quot;Bing Image Search&quot; was the primary source referred by book, offlate course.fast.ai website given alternative DuckDuckGo as this one dont need any key. Please read docs for more information on this. . from fastbook import * from ipywidgets import * . !pip install fastai --upgrade -Uqq . import fastai fastai.__version__ . &#39;2.5.2&#39; . Working with Bears First . First we will learn how to train our first ML model to recognise &quot;type of bear&quot; images then we will go ahead with &quot;Modi Beard Experimentation&quot;. In a way this ensures I am on right track and not messedup anything. . Downloading Images with DuckDuckGo API . bear_types=&#39;grizzly&#39;,&#39;black&#39;,&#39;teddy&#39; # types of bears we want to identify path=Path(&#39;bears&#39;) # creating bears folder path for storing all data if not path.exists(): # bears folder will be created if doesnt exists path.mkdir() for o in bear_types: dest = (path/o) # creating path for each type of bear dest.mkdir(exist_ok=True) # creating child folders under bears results = search_images_ddg(f&#39;{o} bear&#39;, max_images=150) # Collecting URLs from search engine download_images(dest, urls=results) # downloading URLs at specified folders . Checking for failed images . Many times images from the URLs might not be working and we wanted to remomve such images from our training set. Fastai provides good function to remove and unlink the path to those images in our data. . fns=get_image_files(path) # collecting all downloded image files paths (fns - filenames) fns failed=verify_images(fns) # verifying wether image is ok or not failed failed.map(Path.unlink) # unlinking failed images . (#0) [] . Creating DataBlocks and DataLoaders . This will ease the whole process of data handling and channelise the data into model and facilitates both training and validation sets for model. Please explore these videos by Vsihnu Subramaniyam for further undestanding. . bears = DataBlock( blocks=(ImageBlock, CategoryBlock), # This will tell what kind of data we are working with get_items=get_image_files, # it will collect all image files from the path splitter=RandomSplitter(valid_pct=0.2, seed=42), # splits data into train and valid sets get_y=parent_label, # labelling of data for each image item_tfms=RandomResizedCrop(224,0.5), batch_tfms=aug_transforms()) # Resizing of each image to standard size and augmentaton of image #now the above template will be fed with actual data source, here it is &#39;path&#39; dls=bears.dataloaders(path) # data in &#39;path&#39; will be fed to DataBlock through dataloders by filepaths . dls.show_batch() in a way helps us to check wether everything was went well or not. This will show us a batch of images that are going under hammer for training. . dls.show_batch() . Training Bears Model . ResNet18 is a model already trained on ImageNet, and can act like backbone for State of The Art performance on image recognition. we are using resnet18 for this training task using convolutional neural netowrks. Pretrained model is already good at recognising many types of images and here that model will be nudged or finetuned to work on our task of recognising bears. . learn=cnn_learner(dls,resnet18, metrics=error_rate) learn.fine_tune(4,7e-4) . Downloading: &#34;https://download.pytorch.org/models/resnet18-f37072fd.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth . epoch train_loss valid_loss error_rate time . 0 | 1.555692 | 0.435306 | 0.229358 | 00:33 | . epoch train_loss valid_loss error_rate time . 0 | 0.484510 | 0.172394 | 0.064220 | 00:33 | . 1 | 0.361524 | 0.090742 | 0.036697 | 00:34 | . 2 | 0.287017 | 0.073770 | 0.027523 | 00:33 | . 3 | 0.232049 | 0.072706 | 0.027523 | 00:33 | . training seems quite good as validation error rate has decreased to 2.8% i.e accuracy is 97.2%. we can see the confusion matrix and can analyse where exactly model is confusing between different bears. Top losses will show difference between prediction and actual image and also gives the confidence with which it is predicting. . interp=ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(5) . Lets Play With Beard Now . The above code snippet for image recognition is working at State of The Art accuracy and we can apply same understanding on the recognition of Modi Beard, wether is it from 2014 or 2021? For collecting image data we can use same duckduckgo search engine. . !pip install gdown -qq !gdown https://drive.google.com/uc?id=1D3OmFPEKba4b24pfJYcoWRh9ITr-gOMP -O tmp.zip !unzip -q tmp.zip -d ./tmp !rm tmp.zip . Downloading... From: https://drive.google.com/uc?id=1D3OmFPEKba4b24pfJYcoWRh9ITr-gOMP To: /content/tmp.zip 46.7MB [00:00, 128MB/s] . path2=Path(&#39;./tmp&#39;) fns=get_image_files(path2) fns . (#315) [Path(&#39;tmp/modi/2021/00000179.jpg&#39;),Path(&#39;tmp/modi/2021/00000239.jpg&#39;),Path(&#39;tmp/modi/2021/00000044.jpg&#39;),Path(&#39;tmp/modi/2021/00000132.png&#39;),Path(&#39;tmp/modi/2021/00000244.jpg&#39;),Path(&#39;tmp/modi/2021/00000059.jpg&#39;),Path(&#39;tmp/modi/2021/00000165.jpg&#39;),Path(&#39;tmp/modi/2021/00000206.jpg&#39;),Path(&#39;tmp/modi/2021/00000075.jpg&#39;),Path(&#39;tmp/modi/2021/00000112.jpg&#39;)...] . failed2=verify_images(fns) failed2.map(Path.unlink) . (#0) [] . modi = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=RandomResizedCrop(224,0.5), batch_tfms=aug_transforms()) dls2=modi.dataloaders(path2) . dls2.show_batch() . learn2=cnn_learner(dls2,resnet18,metrics=error_rate) # learn2.lr_find() . learn2.fine_tune(10,0.0017) . epoch train_loss valid_loss error_rate time . 0 | 1.136465 | 0.812668 | 0.412698 | 00:07 | . epoch train_loss valid_loss error_rate time . 0 | 0.919497 | 0.840486 | 0.317460 | 00:08 | . 1 | 0.816630 | 0.719514 | 0.285714 | 00:08 | . 2 | 0.730438 | 0.461705 | 0.190476 | 00:08 | . 3 | 0.606138 | 0.337663 | 0.126984 | 00:08 | . 4 | 0.536673 | 0.297072 | 0.126984 | 00:08 | . 5 | 0.464356 | 0.289846 | 0.126984 | 00:08 | . 6 | 0.421824 | 0.260043 | 0.111111 | 00:08 | . 7 | 0.381456 | 0.214176 | 0.079365 | 00:08 | . 8 | 0.364465 | 0.201804 | 0.063492 | 00:07 | . 9 | 0.340084 | 0.192317 | 0.063492 | 00:08 | . As you can see we could achieve nearly 93% accuracy, but this further can be improved by investigating plot losses function of fastai. . interp=ClassificationInterpretation.from_learner(learn2) interp.plot_confusion_matrix() . interp.plot_top_losses(5) . you can very well see that some of the images were no where related to modi, but these are the prime reasons for decreasing efficiency in prediction. Accuracy further can be increased by removing all that data from the files. Now lets build an app to recognise a beard modi from 2014 or 2021. . Building app . we can build and deploy an application on the go and anybody can test the app by uploading an image of modi from their phones or internet, and app will predict wethere the image is from 2014 or 2021. . !gdown https://drive.google.com/uc?id=1cu7wEQP99Y5F_0LJ8BtuC1T0uzBLH_kt learn_inf=load_learner(&#39;export.pkl&#39;) . Downloading... From: https://drive.google.com/uc?id=1cu7wEQP99Y5F_0LJ8BtuC1T0uzBLH_kt To: /content/export.pkl 47.0MB [00:00, 129MB/s] . learn_inf.dls.vocab . [&#39;2014&#39;, &#39;2021&#39;] . btn_upload = widgets.FileUpload() btn_upload . img=PILImage.create(btn_upload.data[-1]) out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) out_pl . pred,pred_idx,probs = learn_inf.predict(img) . lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . btn_run = widgets.Button(description=&#39;Classify&#39;) btn_run def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . btn_upload = widgets.FileUpload() . #hide_output VBox([widgets.Label(&#39;Select your bear!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) .",
            "url": "https://mldurga.github.io/easydl/jupyter/2021/08/04/Fastbook-chapter2.html",
            "relUrl": "/jupyter/2021/08/04/Fastbook-chapter2.html",
            "date": " • Aug 4, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post test check check",
            "content": "This is an attempt to analyse SCADA data for fuel optimisation . Data was collected from SCADA to two minute zoom level and comprises all data related to TPP available in SCADA. All boilers steam generations, power generations, COG , BFG consumptions, import and export data were collected and tried to analyse using data science techniques like tabular and collbarative filtering techniques to suggest optimum power generation and recommended COG, BFG conumptions per hour to each boiler. . from fastbook import * . !mkdir data !7z x /content/gdrive/MyDrive/AI_projects_Data/scada/scada_tpp.7z -odata . 7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21 p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI) Scanning the drive for archives: 0M Scan /content/gdrive/MyDrive/AI_projects_Data/scada/ 1 file, 9204268 bytes (8989 KiB) Extracting archive: /content/gdrive/MyDrive/AI_projects_Data/scada/scada_tpp.7z -- Path = /content/gdrive/MyDrive/AI_projects_Data/scada/scada_tpp.7z Type = 7z Physical Size = 9204268 Headers Size = 1028 Method = LZMA2:12m Solid = + Blocks = 1 0%  34% 3 - scada_tpp/august/01082106.xlsx Everything is Ok Folders: 3 Files: 58 Size: 9730382 Compressed: 9204268 . path=&#39;/content/data/scada_tpp&#39; files=get_files(path, extensions=&#39;.xlsx&#39;) files . (#58) [Path(&#39;/content/data/scada_tpp/august/12082118.xlsx&#39;),Path(&#39;/content/data/scada_tpp/august/12082106.xlsx&#39;),Path(&#39;/content/data/scada_tpp/august/03082118.xlsx&#39;),Path(&#39;/content/data/scada_tpp/august/08082106.xlsx&#39;),Path(&#39;/content/data/scada_tpp/august/03082106.xlsx&#39;),Path(&#39;/content/data/scada_tpp/august/04082118.xlsx&#39;),Path(&#39;/content/data/scada_tpp/august/04082106.xlsx&#39;),Path(&#39;/content/data/scada_tpp/august/09082118.xlsx&#39;),Path(&#39;/content/data/scada_tpp/august/05082106.xlsx&#39;),Path(&#39;/content/data/scada_tpp/august/07082106.xlsx&#39;)...] . frames=[pd.read_excel(x) for x in files] df1=pd.concat(frames) df1.head() . Time ms Pane1-Total COG flow to TPP and PP2 boilers Pane1-Total BFG flow to TPP and PP2 boilers Pane1-GBS CG4 FLOW FOR MIXING WITH BFG Pane1-PBH BLR1 SH ST1 OUTLT HDR FLOW Pane1-PBH BLR2 SH ST1 OUTLT HDR FLOW Pane1-PBH BLR3 SH ST1 OUTLT HDR FLOW Pane1-PBH BLR4 SH ST1 OUTLT HDR FLOW Pane1-PBH BLR5 SH ST1 OUTLT HDR FLOW Pane1-BOILER-6 STEAM FLOW Pane1-PBH BLR1 CG4 GAS SUPPLY HDR FLOW Pane1-PBH BLR2 CG4 GAS SUPPLY HDR FLOW Pane1-PBH BLR3 CG4 GAS SUPPLY HDR FLOW Pane1-PBH BLR4 CG4 GAS SUPPLY HDR FLOW Pane1-PBH BLR5 CG4 GAS SUPPLY HDR FLOW Pane1-PBH BLR5 BFG GAS SUPPLY HDR FLOW Pane1-PBH BLR4 BFG GAS SUPPLY HDR FLOW Pane1-PBH BLR3 BFG GAS SUPPLY HDR FLOW Pane1-PBH BLR1 BFG GAS SUPPLY HDR FLOW Pane1-PBH BLR2 BFG GAS SUPPLY HDR FLOW Pane1-BFG MAIN HDR FLOW BOILER-6 Pane1-Active Power of GSB1 GEN1 Pane1-Active Power of GSB1 GEN2 Pane1-Active Power of GSB1 GEN3 Pane1-MWH Pane1-Import Active Power of TG5 Generation Pane1-Active Power of BPTG GEN1 Pane1-Active Power of BPTG GEN2 Pane1-Active Power of GETS GEN2 Pane1-Active Power of GETS GEN1 Pane1-MWH.1 Pane1-SHRPP TOTAL ACTIVE POWER Pane1-BF-3 TRT MW Pane1-MW AT PP-2 Pane1-MW OF TOTAL GENERATION Pane1-MW OF IMPORT Pane1-MW OF EXPORT Pane1-BF1 SCH COLD BLAST INLT HDR FLOW Pane1-BF2 SCH COLD BLAST INLT HDR FLOW Pane1-COLD BLAST FLOW AT BF3 Pane1-MAIN STEAM HDR FLOW AT BOILER1 OF PP-2 Pane1-MAIN STEAM HDR FLOW AT BOILER2 OF PP-2 Pane1-CO GAS HDR FLOW AT BOILER2 OF PP-2 Pane1-CO GAS HDR FLOW AT BOILER-1 PP-2 Pane1-BF GAS HDR FLOW AT BOILER-1 PP-2 Pane1-BF GAS HDR FLOW AT BOILER2 OF PP-2 . 0 2021-08-12 06:00:00 | 0 | 101551.216600 | 503672.672400 | 6094.000000 | 297.680000 | 239.786667 | 67.866667 | 226.280000 | 281.786667 | -0.0750 | 27268.220700 | 31777.720700 | 62.200002 | 16940.429690 | 16572.412760 | 26525.666670 | 53221.333330 | 266.333333 | 10364.333330 | 1989.000000 | 66529.310340 | 23.970510 | 21.719055 | 50.095828 | 12.305167 | 42.981603 | 6.269791 | 5.401353 | 1.598553 | 3.025691 | 6.600560 | 0.0104 | 6.583310 | 112.362498 | 302.731740 | 112.418999 | 0.0 | 321987.125000 | 293340.593800 | 5368.946940 | 211.880208 | 218.247396 | 4637.333333 | 4169.813333 | 170125.0 | 176427.5 | . 1 2021-08-12 06:02:24 | 0 | 101920.035756 | 508690.087837 | 5835.760000 | 297.123200 | 234.269867 | 69.895467 | 227.912000 | 283.777067 | -0.6942 | 27173.676636 | 32136.614454 | 95.788004 | 17271.954996 | 16217.872136 | 26104.066667 | 54589.493333 | 810.333333 | 10598.253333 | 675.240000 | 67229.619502 | 23.938060 | 21.641155 | 50.053336 | 12.350846 | 42.952255 | 6.235480 | 5.406851 | 1.615383 | 3.170250 | 6.621032 | 0.0104 | 6.769255 | 114.984500 | 305.385534 | 104.595481 | 0.0 | 325898.975048 | 294036.113800 | 5342.191237 | 212.670208 | 218.711146 | 4657.250133 | 4186.197333 | 168970.6 | 175670.3 | . 2 2021-08-12 06:04:48 | 0 | 101648.730895 | 506377.673575 | 5703.253334 | 293.720533 | 234.726933 | 68.906667 | 229.372267 | 288.269867 | 0.9084 | 27278.224528 | 31991.222425 | 165.141003 | 17045.547626 | 16294.896860 | 26615.653336 | 52913.066664 | 838.213333 | 10540.226664 | 3407.026666 | 67254.887036 | 23.955130 | 21.684837 | 50.014844 | 12.288580 | 42.955826 | 6.359599 | 5.489332 | 1.615604 | 2.962657 | 6.591925 | 0.0104 | 6.597269 | 114.336667 | 304.531941 | 100.581682 | 0.0 | 328124.206300 | 296528.403368 | 5072.515404 | 213.901042 | 219.821771 | 4592.281600 | 4179.323734 | 169946.0 | 176668.0 | . 3 2021-08-12 06:07:12 | 0 | 101558.265952 | 505379.559592 | 5889.200000 | 291.907200 | 237.860800 | 69.488000 | 228.953600 | 286.160000 | -0.9828 | 27332.701484 | 31844.999997 | 206.970502 | 17316.013905 | 16360.466096 | 25377.600000 | 52430.720000 | 601.800000 | 9409.840000 | 961.520000 | 67190.406619 | 23.963564 | 21.686560 | 50.072238 | 12.299720 | 42.963882 | 6.406375 | 5.504303 | 1.642149 | 3.016826 | 6.618716 | 0.0104 | 6.259154 | 116.193999 | 306.269362 | 100.044963 | 0.0 | 329036.832484 | 295417.576232 | 5119.948848 | 215.286250 | 220.732500 | 4573.184000 | 4185.894400 | 170085.6 | 176779.5 | . 4 2021-08-12 06:09:36 | 0 | 101369.599980 | 505231.689183 | 5726.960000 | 293.755200 | 236.770133 | 69.133867 | 225.089067 | 285.592000 | -0.9024 | 27440.877470 | 31649.796172 | 115.277335 | 17163.986141 | 16450.292787 | 26477.840000 | 53217.026667 | 509.320000 | 10156.933333 | 1737.400000 | 67048.882282 | 23.964123 | 21.683845 | 50.132416 | 12.295367 | 42.968489 | 6.394841 | 5.490983 | 1.652965 | 2.997148 | 6.595575 | 0.0104 | 6.623072 | 114.748334 | 305.235693 | 104.931444 | 0.0 | 327723.556228 | 301083.050808 | 5267.296407 | 214.584896 | 219.914271 | 4545.996800 | 4196.215467 | 170270.1 | 177122.9 | . df1.drop(columns=&#39;ms&#39;, inplace=True) . columns=[&#39;Time&#39;, &#39;Total_COG&#39;,&#39;Total_BFG&#39;,&#39;COG_inj&#39;,&#39;Blr1_stm_fl&#39;,&#39;Blr2_stm_fl&#39;,&#39;Blr3_stm_fl&#39;,&#39;Blr4_stm_fl&#39;,&#39;Blr5_stm_fl&#39;, &#39;Blr6_stm_fl&#39;, &#39;Blr1_COG_fl&#39;,&#39;Blr2_COG_fl&#39;,&#39;Blr3_COG_fl&#39;,&#39;Blr4_COG_fl&#39;,&#39;Blr5_COG_fl&#39;,&#39;Blr5_BFG_fl&#39;,&#39;Blr4_BFG_fl&#39;,&#39;Blr3_BFG_fl&#39;, &#39;Blr1_BFG_fl&#39;,&#39;Blr2_BFG_fl&#39;,&#39;Blr6_BFG_fl&#39;, &#39;ActPower_TG1&#39;,&#39;ActPower_TG2&#39;, &#39;ActPower_TG3&#39;,&#39;ActPower_TG4&#39;, &#39;ActPower_TG5&#39;, &#39;ActPower_BPTG1&#39;, &#39;ActPower_BPTG2&#39;,&#39;ActPower_GETS2&#39;, &#39;ActPower_GETS1&#39;,&#39;ActPower_COB4&#39;, &#39;ActPower_SHRPP&#39;, &#39;ActPower_TRT&#39;, &#39;ActPower_PP2&#39;, &#39;ActPower_TOTAL&#39;,&#39;IMPORT&#39;, &#39;EXPORT&#39;,&#39;BF1_COLD_BLAST&#39;,&#39;BF2_COLD_BLAST&#39;,&#39;BF3_COLD_BLAST&#39;,&#39;PP2_Blr1_stm_fl&#39;, &#39;PP2_Blr2_stm_fl&#39;,&#39;PP2_Blr2_COG_fl&#39;,&#39;PP2_Blr1_COG_fl&#39;,&#39;PP2_Blr1_BFG_fl&#39;,&#39;PP2_Blr2_BFG_fl&#39;] df1.columns=columns df1=df1.sort_values(&#39;Time&#39;) df1.head() . Time Total_COG Total_BFG COG_inj Blr1_stm_fl Blr2_stm_fl Blr3_stm_fl Blr4_stm_fl Blr5_stm_fl Blr6_stm_fl Blr1_COG_fl Blr2_COG_fl Blr3_COG_fl Blr4_COG_fl Blr5_COG_fl Blr5_BFG_fl Blr4_BFG_fl Blr3_BFG_fl Blr1_BFG_fl Blr2_BFG_fl Blr6_BFG_fl ActPower_TG1 ActPower_TG2 ActPower_TG3 ActPower_TG4 ActPower_TG5 ActPower_BPTG1 ActPower_BPTG2 ActPower_GETS2 ActPower_GETS1 ActPower_COB4 ActPower_SHRPP ActPower_TRT ActPower_PP2 ActPower_TOTAL IMPORT EXPORT BF1_COLD_BLAST BF2_COLD_BLAST BF3_COLD_BLAST PP2_Blr1_stm_fl PP2_Blr2_stm_fl PP2_Blr2_COG_fl PP2_Blr1_COG_fl PP2_Blr1_BFG_fl PP2_Blr2_BFG_fl . 0 2021-07-15 06:00:00 | 55783.931030 | 613856.706900 | 4826.666667 | 255.906667 | 219.546667 | 0.093333 | 210.466667 | 311.400000 | 375.7050 | 8448.833333 | 9250.954102 | 155.500003 | 17062.237630 | 12625.304360 | 4720.333333 | 66492.666670 | 589.333333 | 58349.666670 | 30135.333330 | 101752.586200 | 49.676435 | 48.884159 | 50.472379 | 32.096167 | 47.193966 | 6.559071 | 3.718974 | 2.079202 | 2.779181 | 5.611143 | 0.0104 | 6.887196 | 118.216667 | 383.987081 | 62.332998 | 0.0 | 339936.406300 | 297196.197900 | 5058.680339 | 223.158854 | 226.611979 | 3914.880000 | 4281.066667 | 171787.5 | 183752.5 | . 1 2021-07-15 06:02:24 | 52758.156185 | 610084.660268 | 4919.146667 | 255.669867 | 222.266667 | 1.635733 | 212.015467 | 305.505600 | 380.6514 | 8053.863099 | 8813.688164 | 155.500000 | 15373.507784 | 12199.233896 | 629.453333 | 66381.146667 | 594.773333 | 58238.146667 | 28851.493333 | 101899.839508 | 49.625725 | 49.748761 | 50.421902 | 32.058806 | 47.180336 | 6.612147 | 3.977640 | 2.006621 | 2.743417 | 5.592871 | 0.0104 | 6.683129 | 119.370667 | 386.066363 | 63.416838 | 0.0 | 340968.361276 | 297619.562892 | 4717.864948 | 223.272604 | 228.034479 | 3891.225600 | 4279.889067 | 171333.9 | 183544.9 | . 2 2021-07-15 06:04:48 | 55346.521902 | 611734.286050 | 5006.106666 | 264.706667 | 230.200000 | -0.133867 | 222.016000 | 293.845867 | 390.3450 | 8709.192207 | 9026.152761 | 151.923502 | 16696.190404 | 12680.402912 | 745.733333 | 66618.920000 | 501.160000 | 57579.226664 | 29621.706664 | 102289.393593 | 49.589352 | 51.670523 | 50.425633 | 38.214120 | 47.140453 | 6.602970 | 4.419327 | 2.054747 | 2.756197 | 5.606229 | 0.0104 | 6.693920 | 119.788667 | 394.681095 | 35.350479 | 0.0 | 340421.040388 | 295086.958316 | 4895.239857 | 223.484583 | 229.624688 | 3941.367466 | 4291.959466 | 170963.1 | 183581.1 | . 3 2021-07-15 06:07:12 | 56334.838664 | 610725.224861 | 4325.760000 | 265.393600 | 228.768000 | 0.475200 | 222.422400 | 303.577600 | 390.4110 | 8576.602422 | 9165.947383 | 122.534001 | 17084.163671 | 13038.519492 | 862.920000 | 65308.560000 | 268.600000 | 57804.080000 | 30057.360000 | 101872.914211 | 49.507117 | 51.735209 | 50.459352 | 41.813001 | 47.144430 | 6.608896 | 4.691723 | 2.077191 | 2.748161 | 5.610267 | 0.0104 | 6.927508 | 119.340498 | 398.510631 | 22.270320 | 0.0 | 337018.512540 | 296206.858748 | 5034.670273 | 223.720313 | 229.860938 | 3912.960000 | 4284.800000 | 170668.5 | 183829.2 | . 4 2021-07-15 06:09:36 | 56996.185368 | 615631.248069 | 4898.000000 | 262.877333 | 228.106667 | -1.259200 | 219.472000 | 318.340267 | 391.0362 | 8625.895911 | 9264.171472 | 88.997832 | 17236.138910 | 12933.194260 | 779.280000 | 66509.666667 | 462.400000 | 57962.520000 | 30696.333333 | 102593.246088 | 49.561480 | 51.746712 | 50.435974 | 42.133168 | 47.152799 | 6.617838 | 4.725007 | 2.056070 | 2.800367 | 5.603761 | 0.0104 | 7.011028 | 118.054001 | 397.826609 | 31.359919 | 0.0 | 337715.816692 | 296601.986644 | 5085.494570 | 223.784896 | 230.159271 | 3907.793067 | 4282.670933 | 171779.0 | 185924.6 | .",
            "url": "https://mldurga.github.io/easydl/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://mldurga.github.io/easydl/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "{:height=”200px” width=”100px”} . This website is powered by fastpages [^1]. its test .",
          "url": "https://mldurga.github.io/easydl/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mldurga.github.io/easydl/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}